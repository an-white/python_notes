{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Funciones de Coste.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPZ4hhtK+LM85BkRv6X63Vg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"PWHRPd_8Bb-F"},"source":["**Funcion de Coste**\n","\n","Valores altos prediccion mala valores bajos mejor prediccion\n","\n","**MSE Mean squared error**\n","error medio cuadrado funciona para regresiones y predicciones\n","\n","**Cross entropy**\n","Funciona bien en algoritmos categoricos\n","\n","https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/\n","\n","\n","**Learning rate**\n","\n","Distancia entre los valores de prueba un Learning rate alto provoca un sistema que no converge un learning rate bajo demora mucho tiempo lo que se traduce en un exceso de consumo de potencia de computo\n","\n","**Optimizadores:**\n","Permiten no caer en minimos locales de la funcion lo que permite continuar en busqueda del minimo global de dicha funcion, este me va a decir como optimizar los pesos de los layers ocultos\n","\n","Entre los optmizadores tenemos\n","Momentum\n","Nesterov\n","AdaGrad\n","AdaDelta\n","Adam\n","RMS prop\n","\n","**Descenso del gradiente**\n","\n","https://medium.com/metadatos/todo-lo-que-necesitas-saber-sobre-el-descenso-del-gradiente-aplicado-a-redes-neuronales-19bdbb706a78"]},{"cell_type":"markdown","metadata":{"id":"pL3i8-JBJZE9"},"source":["**Backpropagation**\n","\n","Como distribuir el error?\n","Evaluar la red con el aprendizaje inverso distribuyendo el error de perdidia mediante derivadas parciales \n","\n","https://www.youtube.com/watch?v=eNIqz_noix8\n","\n","https://www.youtube.com/watch?v=M5QHwkkHgAA"]}]}