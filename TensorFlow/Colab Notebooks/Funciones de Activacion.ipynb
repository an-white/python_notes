{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Funciones de Activacion.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMCgEu5T5e00CMx5u+74nk0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"thYlsTzl38px"},"source":["**Funciones de activacion**\n","\n","Existen 2 tipos:\n","\n","**Discretas**\n","Igualemnte que la matematica discreta toma valores discretos o especificos para las variables\n","\n","**Funcion escalonada:**\n","Util para donde solo hay 2 posibles salidas\n","\n","**Funcion sigmoidal:**\n","util para calcular probabilidad y buena para el back propagation porque posee derivada\n","\n","**Funcion RELU**\n","es la mas usada al trabajar con layers ocultos enriquese el valor de entrada en los layers ocultos y la informacion es mejor depurada y permite una rapida convergencia de los modelos\n","\n","**Softmax:**\n","Para clasificacion binaria o multiple, utilizada normalmente en el layer de salida\n","\n","**Continuas**\n","\n","para ver mas funciones de activacion y datos sobre las mismas utilizar\n","https://www.wolframalpha.com/"]}]}